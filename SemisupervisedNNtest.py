import numpy as np
import struct
import os

from sklearn.model_selection import train_test_split
import tensorflow as tf
from tensorflow.keras.datasets import mnist
from tensorflow.keras.models import Sequential
from tensorflow.keras.layers import Dense, Dropout, Flatten
from tensorflow.keras.optimizers import Adam
import streamlit as st
import matplotlib.pyplot as plt

# Cáº¥u HÃ¬nh Streamlit
@st.cache_data  # LÆ°u cache Ä‘á»ƒ trÃ¡nh load láº¡i dá»¯ liá»‡u má»—i láº§n cháº¡y láº¡i Streamlit
def get_sampled_pixels(images, sample_size=100_000):
    return np.random.choice(images.flatten(), sample_size, replace=False)

@st.cache_data  # Cache danh sÃ¡ch áº£nh ngáº«u nhiÃªn
def get_random_indices(num_images, total_images):
    return np.random.randint(0, total_images, size=num_images)
# Cáº¥u hÃ¬nh Streamlit    
# st.set_page_config(page_title="PhÃ¢n loáº¡i áº£nh", layout="wide")
# Äá»‹nh nghÄ©a hÃ m Ä‘á»ƒ Ä‘á»c file .idx
def load_mnist_images(filename):
    with open(filename, 'rb') as f:
        magic, num, rows, cols = struct.unpack('>IIII', f.read(16))
        images = np.fromfile(f, dtype=np.uint8).reshape(num, rows, cols)
    return images
def load_mnist_labels(filename):
    with open(filename, 'rb') as f:
        magic, num = struct.unpack('>II', f.read(8))
        labels = np.fromfile(f, dtype=np.uint8)
    return labels   

# HÃ m Ä‘á»ƒ láº¥y 1% dá»¯ liá»‡u ban Ä‘áº§u cho má»—i class
def get_initial_train_data(x_train, y_train, percent=0.01):
    initial_x_train = []
    initial_y_train = []
    
    for class_label in range(10):
        class_indices = np.where(np.argmax(y_train, axis=1) == class_label)[0]
        num_samples = int(len(class_indices) * percent)
        selected_indices = np.random.choice(class_indices, num_samples, replace=False)
        
        initial_x_train.append(x_train[selected_indices])
        initial_y_train.append(y_train[selected_indices])
    
    initial_x_train = np.concatenate(initial_x_train)
    initial_y_train = np.concatenate(initial_y_train)
    
    return initial_x_train, initial_y_train

# HÃ m Ä‘á»ƒ xÃ¢y dá»±ng model Neural Network
def build_model():
    model = Sequential([
        Flatten(input_shape=(28, 28)),
        Dense(128, activation='relu'),
        Dropout(0.2),
        Dense(10, activation='softmax')
    ])
    model.compile(optimizer=Adam(), loss='categorical_crossentropy', metrics=['accuracy'])
    return model

# HÃ m chÃ­nh Ä‘á»ƒ thá»±c hiá»‡n Pseudo Labelling
def pseudo_labelling(x_train, y_train, x_test, y_test, threshold, max_iter):
    # Láº¥y 1% dá»¯ liá»‡u ban Ä‘áº§u
    initial_x_train, initial_y_train = get_initial_train_data(x_train, y_train)
    
    # Huáº¥n luyá»‡n model ban Ä‘áº§u
    model = build_model()
    history = model.fit(initial_x_train, initial_y_train, epochs=10, batch_size=32, validation_split=0.2, verbose=0)
    
    # Hiá»ƒn thá»‹ káº¿t quáº£ ban Ä‘áº§u
    st.write("### Káº¿t quáº£ huáº¥n luyá»‡n ban Ä‘áº§u")
    st.write(f"Äá»™ chÃ­nh xÃ¡c trÃªn táº­p validation: {history.history['val_accuracy'][-1]:.4f}")
    
    # Láº·p láº¡i quÃ¡ trÃ¬nh Pseudo Labelling
    for i in range(max_iter):
        st.write(f"### Láº§n láº·p {i + 1}")
        
        # Dá»± Ä‘oÃ¡n nhÃ£n cho pháº§n dá»¯ liá»‡u cÃ²n láº¡i
        remaining_x_train = np.delete(x_train, np.where(np.isin(x_train, initial_x_train))[0], axis=0)
        predictions = model.predict(remaining_x_train)
        
        # GÃ¡n nhÃ£n giáº£ cho cÃ¡c máº«u cÃ³ Ä‘á»™ tin cáº­y cao
        pseudo_labels = np.argmax(predictions, axis=1)
        confidences = np.max(predictions, axis=1)
        high_confidence_indices = np.where(confidences > threshold)[0]
        
        pseudo_x_train = remaining_x_train[high_confidence_indices]
        pseudo_y_train = tf.keras.utils.to_categorical(pseudo_labels[high_confidence_indices], 10)
        
        # Cáº­p nháº­t táº­p dá»¯ liá»‡u huáº¥n luyá»‡n
        initial_x_train = np.concatenate([initial_x_train, pseudo_x_train])
        initial_y_train = np.concatenate([initial_y_train, pseudo_y_train])
        
        # Huáº¥n luyá»‡n láº¡i model
        model = build_model()
        history = model.fit(initial_x_train, initial_y_train, epochs=10, batch_size=32, validation_split=0.2, verbose=0)
        
        # Hiá»ƒn thá»‹ káº¿t quáº£
        st.write(f"Äá»™ chÃ­nh xÃ¡c trÃªn táº­p validation sau láº§n láº·p {i + 1}: {history.history['val_accuracy'][-1]:.4f}")
        st.write(f"Sá»‘ lÆ°á»£ng máº«u Ä‘Æ°á»£c gÃ¡n nhÃ£n giáº£: {len(pseudo_x_train)}")
        
        # Hiá»ƒn thá»‹ má»™t sá»‘ áº£nh Ä‘Æ°á»£c gÃ¡n nhÃ£n giáº£
        st.write("Má»™t sá»‘ áº£nh Ä‘Æ°á»£c gÃ¡n nhÃ£n giáº£:")
        fig, axes = plt.subplots(1, 5, figsize=(10, 2))
        for idx, ax in enumerate(axes):
            ax.imshow(pseudo_x_train[idx], cmap='gray')
            ax.set_title(f"Label: {np.argmax(pseudo_y_train[idx])}")
            ax.axis('off')
        st.pyplot(fig)
    
    # ÄÃ¡nh giÃ¡ model trÃªn táº­p test
    test_loss, test_acc = model.evaluate(x_test, y_test, verbose=0)
    st.write("### Káº¿t quáº£ cuá»‘i cÃ¹ng trÃªn táº­p test")
    st.write(f"Äá»™ chÃ­nh xÃ¡c trÃªn táº­p test: {test_acc:.4f}")

# Giao diá»‡n Streamlit
def run_PseudoLabellingt_app():
    # Äá»‹nh nghÄ©a Ä‘Æ°á»ng dáº«n Ä‘áº¿n cÃ¡c file MNIST
    dataset_path = os.path.dirname(os.path.abspath(__file__)) 
    train_images_path = os.path.join(dataset_path, "train-images.idx3-ubyte")
    train_labels_path = os.path.join(dataset_path, "train-labels.idx1-ubyte")
    test_images_path = os.path.join(dataset_path, "t10k-images.idx3-ubyte")
    test_labels_path = os.path.join(dataset_path, "t10k-labels.idx1-ubyte")

    # Táº£i dá»¯ liá»‡u
    train_images = load_mnist_images(train_images_path)
    train_labels = load_mnist_labels(train_labels_path)
    test_images = load_mnist_images(test_images_path)
    test_labels = load_mnist_labels(test_labels_path)

    # Chuyá»ƒn Ä‘á»•i dá»¯ liá»‡u thÃ nh vector 1 chiá»u
    X = np.concatenate((train_images, test_images), axis=0)  # Gá»™p toÃ n bá»™ dá»¯ liá»‡u
    y = np.concatenate((train_labels, test_labels), axis=0)
    X = X.reshape(X.shape[0], -1)  # Chuyá»ƒn thÃ nh vector 1 chiá»u

    # Cho phÃ©p ngÆ°á»i dÃ¹ng chá»n tá»· lá»‡ validation vÃ  test
    test_size = st.slider("ğŸ”¹ Chá»n % tá»· lá»‡ táº­p test", min_value=10, max_value=50, value=20, step=1) / 100
            
    x_train, y_train, x_test, y_test = train_test_split(X, y, test_size=test_size, random_state=42)
    
    st.title("Pseudo Labelling vá»›i Neural Network trÃªn táº­p dá»¯ liá»‡u MNIST")
    
    # Thiáº¿t láº­p cÃ¡c tham sá»‘
    threshold = st.sidebar.slider("NgÆ°á»¡ng quyáº¿t Ä‘á»‹nh", min_value=0.8, max_value=0.99, value=0.95, step=0.01)
    max_iter = st.sidebar.slider("Sá»‘ láº§n láº·p", min_value=1, max_value=10, value=5, step=1)
    
    # Báº¯t Ä‘áº§u quÃ¡ trÃ¬nh Pseudo Labelling
    if st.button("Báº¯t Ä‘áº§u"):
        pseudo_labelling(x_train, y_train, x_test, y_test, threshold, max_iter)

# Cháº¡y á»©ng dá»¥ng Streamlit
if __name__ == "__main__":
    run_PseudoLabellingt_app()