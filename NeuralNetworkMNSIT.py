import time
from sklearn.neural_network import MLPClassifier
import streamlit as st
import os
import cv2
import numpy as np
import pandas as pd
import pickle
import seaborn as sns
import random
import struct
from sklearn.datasets import fetch_openml, load_iris
import mlflow
import matplotlib.pyplot as plt
from streamlit_drawable_canvas import st_canvas
from sklearn.model_selection import train_test_split
from sklearn import datasets
from sklearn.preprocessing import StandardScaler
from sklearn.tree import DecisionTreeClassifier,plot_tree
from sklearn.svm import SVC
from sklearn.metrics import accuracy_score, classification_report, ConfusionMatrixDisplay
from PIL import Image
from sklearn.model_selection import KFold
from collections import Counter
from mlflow.tracking import MlflowClient
from streamlit_drawable_canvas import st_canvas
from tensorflow.keras import layers, models, callbacks, optimizers
from tensorflow.keras.callbacks import EarlyStopping, ModelCheckpoint
from tensorflow.keras.models import load_model
from sklearn.model_selection import train_test_split, StratifiedKFold
from mlflow.models.signature import infer_signature
from tensorflow.keras.models import Sequential
from tensorflow import keras

def preprocess_canvas_image(canvas_result):
    if canvas_result.image_data is not None:
        img = Image.fromarray(canvas_result.image_data[:, :, 0].astype(np.uint8))
        img = img.resize((28, 28)).convert("L")  # Resize v√† chuy·ªÉn th√†nh grayscale
        img = np.array(img, dtype=np.float32) / 255.0  # Chu·∫©n h√≥a v·ªÅ [0, 1]
        return img.reshape(1, -1)  # Chuy·ªÉn th√†nh vector 1D
    return None

def run_NeuralNetwork_app():
    @st.cache_data  # L∆∞u cache ƒë·ªÉ tr√°nh load l·∫°i d·ªØ li·ªáu m·ªói l·∫ßn ch·∫°y l·∫°i Streamlit
    def get_sampled_pixels(images, sample_size=100_000):
        return np.random.choice(images.flatten(), sample_size, replace=False)

    @st.cache_data  # Cache danh s√°ch ·∫£nh ng·∫´u nhi√™n
    def get_random_indices(num_images, total_images):
        return np.random.randint(0, total_images, size=num_images)

    # C·∫•u h√¨nh Streamlit    
    # st.set_page_config(page_title="Ph√¢n lo·∫°i ·∫£nh", layout="wide")
    # ƒê·ªãnh nghƒ©a h√†m ƒë·ªÉ ƒë·ªçc file .idx
    def load_mnist_images(filename):
        with open(filename, 'rb') as f:
            magic, num, rows, cols = struct.unpack('>IIII', f.read(16))
            images = np.fromfile(f, dtype=np.uint8).reshape(num, rows, cols)
        return images

    def load_mnist_labels(filename):
        with open(filename, 'rb') as f:
            magic, num = struct.unpack('>II', f.read(8))
            labels = np.fromfile(f, dtype=np.uint8)
        return labels

    mlflow_tracking_uri = st.secrets["MLFLOW_TRACKING_URI"]
    mlflow_username = st.secrets["MLFLOW_TRACKING_USERNAME"]
    mlflow_password = st.secrets["MLFLOW_TRACKING_PASSWORD"]
    
    # Thi·∫øt l·∫≠p bi·∫øn m√¥i tr∆∞·ªùng
    os.environ["MLFLOW_TRACKING_URI"] = mlflow_tracking_uri
    os.environ["MLFLOW_TRACKING_USERNAME"] = mlflow_username
    os.environ["MLFLOW_TRACKING_PASSWORD"] = mlflow_password
    
    # Thi·∫øt l·∫≠p MLflow (ƒê·∫∑t sau khi mlflow_tracking_uri ƒë√£ c√≥ gi√° tr·ªã)
    mlflow.set_tracking_uri(mlflow_tracking_uri)



    # ƒê·ªãnh nghƒ©a ƒë∆∞·ªùng d·∫´n ƒë·∫øn c√°c file MNIST
    dataset_path = os.path.dirname(os.path.abspath(__file__)) 
    train_images_path = os.path.join(dataset_path, "train-images.idx3-ubyte")
    train_labels_path = os.path.join(dataset_path, "train-labels.idx1-ubyte")
    test_images_path = os.path.join(dataset_path, "t10k-images.idx3-ubyte")
    test_labels_path = os.path.join(dataset_path, "t10k-labels.idx1-ubyte")

    # T·∫£i d·ªØ li·ªáu
    train_images = load_mnist_images(train_images_path)
    train_labels = load_mnist_labels(train_labels_path)
    test_images = load_mnist_images(test_images_path)
    test_labels = load_mnist_labels(test_labels_path)

    # Giao di·ªán Streamlit
    st.title("üì∏ Ph√¢n lo·∫°i ·∫£nh MNIST v·ªõi Streamlit")
    tabs = st.tabs([
        "Th√¥ng tin d·ªØ li·ªáu",
        "Th√¥ng tin",
        "X·ª≠ l√≠ d·ªØ li·ªáu",
        "Hu·∫•n luy·ªán m√¥ h√¨nh",
        "Demo d·ª± ƒëo√°n file ·∫£nh",
        "Demo d·ª± ƒëo√°n Vi·∫øt Tay",
        "Th√¥ng tin & Mlflow",
    ])
    # tab_info, tab_load, tab_preprocess, tab_split,  tab_demo, tab_log_info = tabs
    tab_info,tab_note,tab_load, tab_preprocess,  tab_demo, tab_demo_2 ,tab_mlflow= tabs

    # with st.expander("üñºÔ∏è D·ªØ li·ªáu ban ƒë·∫ßu", expanded=True):
    with tab_info:
        with st.expander("**Th√¥ng tin d·ªØ li·ªáu**", expanded=True):
            st.markdown(
                '''
                **MNIST** l√† phi√™n b·∫£n ƒë∆∞·ª£c ch·ªânh s·ª≠a t·ª´ b·ªô d·ªØ li·ªáu NIST g·ªëc c·ªßa Vi·ªán Ti√™u chu·∫©n v√† C√¥ng ngh·ªá Qu·ªëc gia Hoa K·ª≥.  
                B·ªô d·ªØ li·ªáu ban ƒë·∫ßu g·ªìm c√°c ch·ªØ s·ªë vi·∫øt tay t·ª´ nh√¢n vi√™n b∆∞u ƒëi·ªán v√† h·ªçc sinh trung h·ªçc.  

                C√°c nh√† nghi√™n c·ª©u **Yann LeCun, Corinna Cortes, v√† Christopher Burges** ƒë√£ x·ª≠ l√Ω, chu·∫©n h√≥a v√† chuy·ªÉn ƒë·ªïi b·ªô d·ªØ li·ªáu n√†y th√†nh **MNIST** ƒë·ªÉ d·ªÖ d√†ng s·ª≠ d·ª•ng h∆°n cho c√°c b√†i to√°n nh·∫≠n d·∫°ng ch·ªØ s·ªë vi·∫øt tay.
                '''
            )
            # image = Image.open(r'C:\Users\Dell\OneDrive\Pictures\Documents\Code\python\OpenCV\HMVPYTHON\App\image.png')

            # G·∫Øn ·∫£nh v√†o Streamlit v√† ch·ªânh k√≠ch th∆∞·ªõc
            # st.image(image, caption='M√¥ t·∫£ ·∫£nh', width=600) 
            # ƒê·∫∑c ƒëi·ªÉm c·ªßa b·ªô d·ªØ li·ªáu
        with st.expander("**ƒê·∫∑c ƒëi·ªÉm c·ªßa b·ªô d·ªØ li·ªáu**", expanded=True):
            st.markdown(
                '''
                - **S·ªë l∆∞·ª£ng ·∫£nh:** 70.000 ·∫£nh ch·ªØ s·ªë vi·∫øt tay  
                - **K√≠ch th∆∞·ªõc ·∫£nh:** M·ªói ·∫£nh c√≥ k√≠ch th∆∞·ªõc 28x28 pixel  
                - **C∆∞·ªùng ƒë·ªô ƒëi·ªÉm ·∫£nh:** T·ª´ 0 (m√†u ƒëen) ƒë·∫øn 255 (m√†u tr·∫Øng)  
                - **D·ªØ li·ªáu nh√£n:** M·ªói ·∫£nh ƒëi k√®m v·ªõi m·ªôt nh√£n s·ªë t·ª´ 0 ƒë·∫øn 9  
                '''
            )
            st.write(f"üîç S·ªë l∆∞·ª£ng ·∫£nh hu·∫•n luy·ªán: `{train_images.shape[0]}`")
            st.write(f"üîç S·ªë l∆∞·ª£ng ·∫£nh ki·ªÉm tra: `{test_images.shape[0]}`")

        with st.expander("**Hi·ªÉn th·ªã s·ªë l∆∞·ª£ng m·∫´u c·ªßa t·ª´ng ch·ªØ s·ªë t·ª´ 0 ƒë·∫øn 9 trong t·∫≠p hu·∫•n luy·ªán**", expanded=True):
            label_counts = pd.Series(train_labels).value_counts().sort_index()

            # # Hi·ªÉn th·ªã bi·ªÉu ƒë·ªì c·ªôt
            # st.subheader("üìä Bi·ªÉu ƒë·ªì s·ªë l∆∞·ª£ng m·∫´u c·ªßa t·ª´ng ch·ªØ s·ªë")
            # st.bar_chart(label_counts)

            # Hi·ªÉn th·ªã b·∫£ng d·ªØ li·ªáu d∆∞·ªõi bi·ªÉu ƒë·ªì
            st.subheader("üìã S·ªë l∆∞·ª£ng m·∫´u cho t·ª´ng ch·ªØ s·ªë")
            df_counts = pd.DataFrame({"Ch·ªØ s·ªë": label_counts.index, "S·ªë l∆∞·ª£ng m·∫´u": label_counts.values})
            st.dataframe(df_counts)


            st.subheader("Ch·ªçn ng·∫´u nhi√™n 10 ·∫£nh t·ª´ t·∫≠p hu·∫•n luy·ªán ƒë·ªÉ hi·ªÉn th·ªã")
            num_images = 10
            random_indices = random.sample(range(len(train_images)), num_images)
            fig, axes = plt.subplots(1, num_images, figsize=(10, 5))

            for ax, idx in zip(axes, random_indices):
                ax.imshow(train_images[idx], cmap='gray')
                ax.axis("off")
                ax.set_title(f"Label: {train_labels[idx]}")

            st.pyplot(fig)
        with st.expander("**Ki·ªÉm tra h√¨nh d·∫°ng c·ªßa t·∫≠p d·ªØ li·ªáu**", expanded=True):    
            # Ki·ªÉm tra h√¨nh d·∫°ng c·ªßa t·∫≠p d·ªØ li·ªáu
            st.write("üîç H√¨nh d·∫°ng t·∫≠p hu·∫•n luy·ªán:", train_images.shape)
            st.write("üîç H√¨nh d·∫°ng t·∫≠p ki·ªÉm tra:", test_images.shape)
            st.write("**Chu·∫©n h√≥a d·ªØ li·ªáu (ƒë∆∞a gi√° tr·ªã pixel v·ªÅ kho·∫£ng 0-1)**")
            # Chu·∫©n h√≥a d·ªØ li·ªáu
            train_images = train_images.astype("float32") / 255.0
            test_images = test_images.astype("float32") / 255.0

            # Hi·ªÉn th·ªã th√¥ng b√°o sau khi chu·∫©n h√≥a
            st.success("‚úÖ D·ªØ li·ªáu ƒë√£ ƒë∆∞·ª£c chu·∫©n h√≥a v·ªÅ kho·∫£ng [0,1].") 

            # Hi·ªÉn th·ªã b·∫£ng d·ªØ li·ªáu ƒë√£ chu·∫©n h√≥a (d·∫°ng s·ªë)
            num_samples = 5  # S·ªë l∆∞·ª£ng m·∫´u hi·ªÉn th·ªã
            df_normalized = pd.DataFrame(train_images[:num_samples].reshape(num_samples, -1))  

            
            sample_size = 10_000  
            pixel_sample = np.random.choice(train_images.flatten(), sample_size, replace=False)
            if "train_images" not in st.session_state:
                st.session_state.train_images = train_images
                st.session_state.train_labels = train_labels
                st.session_state.test_images = test_images
                st.session_state.test_labels = test_labels


    with tab_note:
        with st.expander("**Th√¥ng tin m√¥ h√¨nh**", expanded=True):
        # Assume model_option1 is selected from somewhere in the app
            st.markdown("""
                    ### Neural Network (NN)
                    """) 
            st.markdown("---")        
            st.markdown("""            
            ### Kh√°i Ni·ªám:  
            **Neural Network (NN)**:
            - L√† m·ªôt m√¥ h√¨nh t√≠nh to√°n l·∫•y c·∫£m h·ª©ng t·ª´ c·∫•u tr√∫c v√† ch·ª©c nƒÉng c·ªßa m·∫°ng l∆∞·ªõi th·∫ßn kinh sinh h·ªçc. N√≥ ƒë∆∞·ª£c t·∫°o th√†nh t·ª´ c√°c n√∫t k·∫øt n·ªëi v·ªõi nhau, hay c√≤n g·ªçi l√† n∆°-ron nh√¢n t·∫°o, ƒë∆∞·ª£c s·∫Øp x·∫øp th√†nh c√°c l·ªõp.
            - √ù t∆∞·ªüng ch√≠nh c·ªßa **Neural Network** l√† t·∫°o ra m·ªôt m√¥ h√¨nh t√≠nh to√°n c√≥ kh·∫£ nƒÉng h·ªçc h·ªèi v√† x·ª≠ l√Ω th√¥ng tin gi·ªëng nh∆∞ b·ªô n√£o con ng∆∞·ªùi.
            """)

            st.markdown("---")        
                       
            st.write("### M√¥ H√¨nh T·ªïng Qu√°t:")   
            st.image("imgnn/modelnn.png",use_container_width ="auto")
            st.markdown(""" 
            - Layer ƒë·∫ßu ti√™n l√† input layer, c√°c layer ·ªü gi·ªØa ƒë∆∞·ª£c g·ªçi l√† hidden layer, layer cu·ªëi c√πng ƒë∆∞·ª£c g·ªçi l√† output layer. C√°c h√¨nh tr√≤n ƒë∆∞·ª£c g·ªçi l√† node.
            - M·ªói m√¥ h√¨nh lu√¥n c√≥ 1 input layer, 1 output layer, c√≥ th·ªÉ c√≥ ho·∫∑c kh√¥ng c√°c hidden layer. T·ªïng s·ªë layer trong m√¥ h√¨nh ƒë∆∞·ª£c quy ∆∞·ªõc l√† s·ªë layer - 1 (Kh√¥ng t√≠nh input layer).
            - M·ªói node trong hidden layer v√† output layer :
                - Li√™n k·∫øt v·ªõi t·∫•t c·∫£ c√°c node ·ªü layer tr∆∞·ªõc ƒë√≥ v·ªõi c√°c h·ªá s·ªë w ri√™ng.
                - M·ªói node c√≥ 1 h·ªá s·ªë bias b ri√™ng.
                - Di·ªÖn ra 2 b∆∞·ªõc: t√≠nh t·ªïng linear v√† √°p d·ª•ng activation function.
            """)

            st.markdown("---")          
            st.markdown("""
            ### Nguy√™n l√Ω ho·∫°t ƒë·ªông:  
            - D·ªØ li·ªáu ƒë·∫ßu v√†o ƒë∆∞·ª£c ƒë∆∞a v√†o l·ªõp ƒë·∫ßu v√†o.
            - M·ªói n∆°-ron trong l·ªõp ·∫©n nh·∫≠n t√≠n hi·ªáu t·ª´ c√°c n∆°-ron ·ªü l·ªõp tr∆∞·ªõc ƒë√≥, x·ª≠ l√Ω t√≠n hi·ªáu v√† chuy·ªÉn ti·∫øp k·∫øt qu·∫£ ƒë·∫øn c√°c n∆°-ron ·ªü l·ªõp ti·∫øp theo.
            - Qu√° tr√¨nh n√†y ti·∫øp t·ª•c cho ƒë·∫øn khi d·ªØ li·ªáu ƒë·∫øn l·ªõp ƒë·∫ßu ra.
            - K·∫øt qu·∫£ ƒë·∫ßu ra ƒë∆∞·ª£c t·∫°o ra d·ª±a tr√™n c√°c t√≠n hi·ªáu nh·∫≠n ƒë∆∞·ª£c t·ª´ l·ªõp ·∫©n cu·ªëi c√πng.
            """)           
            st.markdown("---")
            st.markdown("""  
            ### √Åp d·ª•ng v√†o ng·ªØ c·∫£nh Neural Network v·ªõi MNIST:  
            - **MNIST (Modified National Institute of Standards and Technology database)** l√† m·ªôt b·ªô d·ªØ li·ªáu kinh ƒëi·ªÉn trong lƒ©nh v·ª±c h·ªçc m√°y, ƒë·∫∑c bi·ªát l√† trong vi·ªác √°p d·ª•ng m·∫°ng n∆°-ron. N√≥ bao g·ªìm 70.000 ·∫£nh x√°m c·ªßa ch·ªØ s·ªë vi·∫øt tay (t·ª´ 0 ƒë·∫øn 9), ƒë∆∞·ª£c chia th√†nh 60.000 ·∫£nh hu·∫•n luy·ªán v√† 10.000 ·∫£nh ki·ªÉm tra.
            - M·ª•c ti√™u c·ªßa b√†i to√°n l√† ph√¢n lo·∫°i ch√≠nh x√°c ch·ªØ s·ªë t·ª´ 0 ƒë·∫øn 9 d·ª±a tr√™n ·∫£nh ƒë·∫ßu v√†o.
            - C√≥ nhi·ªÅu c√°ch ƒë·ªÉ √°p d·ª•ng m·∫°ng n∆°-ron cho b√†i to√°n ph√¢n lo·∫°i ch·ªØ s·ªë vi·∫øt tay tr√™n MNIST. D∆∞·ªõi ƒë√¢y l√† m·ªôt s·ªë ph∆∞∆°ng ph√°p ph·ªï bi·∫øn:
                - **Multi-Layer Perceptron (MLP)**: M·ªôt m√¥ h√¨nh m·∫°ng n∆°-ron s√¢u v·ªõi nhi·ªÅu l·ªõp ·∫©n.
                - **Convolutional Neural Network (CNN)**: M·ªôt m√¥ h√¨nh m·∫°ng n∆°-ron s√¢u ƒë∆∞·ª£c thi·∫øt k·∫ø ƒë·∫∑c bi·ªát cho vi·ªác x·ª≠ l√Ω ·∫£nh.
                - **Recurrent Neural Network (RNN)**: M·ªôt m√¥ h√¨nh m·∫°ng n∆°-ron s√¢u ƒë∆∞·ª£c thi·∫øt k·∫ø cho d·ªØ li·ªáu chu·ªói.
            """)
                


    with tab_load:
        with st.expander("**Ph√¢n chia d·ªØ li·ªáu**", expanded=True):    

            # Ki·ªÉm tra n·∫øu d·ªØ li·ªáu ƒë√£ ƒë∆∞·ª£c load
            if "train_images" in st.session_state:
                # L·∫•y d·ªØ li·ªáu t·ª´ session_state
                train_images = st.session_state.train_images
                train_labels = st.session_state.train_labels
                test_images = st.session_state.test_images
                test_labels = st.session_state.test_labels

                # Chuy·ªÉn ƒë·ªïi d·ªØ li·ªáu th√†nh vector 1 chi·ªÅu
                X = np.concatenate((train_images, test_images), axis=0)  # G·ªôp to√†n b·ªô d·ªØ li·ªáu
                y = np.concatenate((train_labels, test_labels), axis=0)
                X = X.reshape(X.shape[0], -1)  # Chuy·ªÉn th√†nh vector 1 chi·ªÅu

                with mlflow.start_run():

                    # Cho ph√©p ng∆∞·ªùi d√πng ch·ªçn t·ª∑ l·ªá validation v√† test
                    test_size = st.slider("üîπ Ch·ªçn % t·ª∑ l·ªá t·∫≠p test", min_value=10, max_value=50, value=20, step=5) / 100
                    val_size = st.slider("üîπ Ch·ªçn % t·ª∑ l·ªá t·∫≠p validation (trong ph·∫ßn train)", min_value=10, max_value=50, value=20, step=5) / 100

                    X_temp, X_test, y_temp, y_test = train_test_split(X, y, test_size=test_size, random_state=42)
                    val_size_adjusted = val_size / (1 - test_size)  # ƒêi·ªÅu ch·ªânh t·ª∑ l·ªá val cho ph·∫ßn c√≤n l·∫°i
                    X_train, X_val, y_train, y_val = train_test_split(X_temp, y_temp, test_size=val_size_adjusted, random_state=42)

                    # T√≠nh t·ª∑ l·ªá th·ª±c t·∫ø c·ªßa t·ª´ng t·∫≠p
                    total_samples = X.shape[0]
                    test_percent = (X_test.shape[0] / total_samples) * 100
                    val_percent = (X_val.shape[0] / total_samples) * 100
                    train_percent = (X_train.shape[0] / total_samples) * 100
                st.write(f"üìä **T·ª∑ l·ªá ph√¢n chia**: Test={test_percent:.0f}%, Validation={val_percent:.0f}%, Train={train_percent:.0f}%")
                st.write("‚úÖ D·ªØ li·ªáu ƒë√£ ƒë∆∞·ª£c x·ª≠ l√Ω v√† chia t√°ch.")
                st.write(f"üîπ K√≠ch th∆∞·ªõc t·∫≠p hu·∫•n luy·ªán: `{X_train.shape}`")
                st.write(f"üîπ K√≠ch th∆∞·ªõc t·∫≠p validation: `{X_val.shape}`")
                st.write(f"üîπ K√≠ch th∆∞·ªõc t·∫≠p ki·ªÉm tra: `{X_test.shape}`")
            else:
                st.error("üö® D·ªØ li·ªáu ch∆∞a ƒë∆∞·ª£c n·∫°p. H√£y ƒë·∫£m b·∫£o `train_images`, `train_labels` v√† `test_images` ƒë√£ ƒë∆∞·ª£c t·∫£i tr∆∞·ªõc khi ch·∫°y.")



    # 3Ô∏è‚É£ HU·∫§N LUY·ªÜN M√î H√åNH
    with tab_preprocess:
        with st.expander("**Hu·∫•n luy·ªán Neural Network**", expanded=True):
            
            train_data_size = st.slider("S·ªë l∆∞·ª£ng d·ªØ li·ªáu d√πng ƒë·ªÉ train model", 100, len(X_train), len(X_train), step=100)
            X_train = X_train[:train_data_size]
            y_train = y_train[:train_data_size]

            # Chu·∫©n h√≥a d·ªØ li·ªáu
            X_train = X_train / 255.0
            X_val = X_val / 255.0
            X_test = X_test / 255.0
            
            # L·ª±a ch·ªçn tham s·ªë hu·∫•n luy·ªán
            k_folds = st.slider("S·ªë fold cho Cross-Validation:", 3, 10, 5)
            
            num_layers = st.slider("S·ªë l·ªõp ·∫©n:", 1, 5, 2)

            epochs = st.slider("S·ªë l·∫ßn l·∫∑p t·ªëi ƒëa", 2, 50, 5)

            learning_rate_init = st.slider("T·ªëc ƒë·ªô h·ªçc", 0.001, 0.1, 0.01, step = 0.001, format="%.3f")

            activation = st.selectbox("H√†m k√≠ch ho·∫°t:", ["relu", "sigmoid", "tanh"])

            num_neurons = st.selectbox("S·ªë neuron m·ªói l·ªõp:", [32, 64, 128, 256], index=0)

            optimizer = st.selectbox("Ch·ªçn h√†m t·ªëi ∆∞u", ["adam", "sgd", "lbfgs"])

            loss_fn = "sparse_categorical_crossentropy"

            if st.button("‚èπÔ∏è Hu·∫•n luy·ªán m√¥ h√¨nh"):
                with st.spinner("üîÑ ƒêang hu·∫•n luy·ªán..."):
                    with mlflow.start_run():
                        
                        # progress_bar = st.progress(0)
                        # history = None
                        # start_time = time.time()
                        # # Hu·∫•n luy·ªán m√¥ h√¨nh v·ªõi callback ƒë·ªÉ c·∫≠p nh·∫≠t progress bar
                        # class ProgressCallback(tf.keras.callbacks.Callback):
                        #     def on_epoch_end(self, epoch, logs=None):
                        #         progress = (epoch + 1) / epochs * 100
                        #         progress_bar.progress(int(progress))
                        #         st.write(f"Epoch {epoch+1}/{epochs}: {int(progress)}% ho√†n th√†nh")
                        #         st.write(f"Loss: {logs['loss']:.4f}, Accuracy: {logs['accuracy']:.4f}")

                        # # Hu·∫•n luy·ªán m√¥ h√¨nh
                        # # early_stopping = EarlyStopping(monitor='val_loss', patience=5, min_delta=0.001)
                        # # model_checkpoint = ModelCheckpoint('best_model.h5', monitor='val_loss', save_best_only=True, mode='min')
                        # history = cnn.fit(X_train, y_train,
                        #                 epochs=epochs,
                        #                 batch_size=batch_size,
                        #                 validation_data=(X_val, y_val),
                        #                 verbose=1,
                        #                 callbacks=[ProgressCallback()])


                        # # Ho√†n th√†nh progress bar
                        # progress_bar.progress(100)

                        # end_time = time.time()
                        # training_time = end_time - start_time

                        # # Ghi log v·ªõi MLflow
                        # mlflow.log_param("epochs", epochs)
                        # mlflow.log_param("batch_size", batch_size)
                        # mlflow.log_param("optimizer", optimizer)
                        # mlflow.log_metric("train_accuracy", history.history['accuracy'][-1])
                        # mlflow.log_metric("val_accuracy", history.history['val_accuracy'][-1])
                        # mlflow.log_metric("final_train_loss", history.history['loss'][-1])
                        # mlflow.log_metric("final_val_loss", history.history['val_loss'][-1])

                        # y_pred = cnn.predict(X_test)
                        # y_pred_class = np.argmax(y_pred, axis=1)
                        # accuracy = accuracy_score(y_test, y_pred_class)

                        mlflow.log_params({"num_layers": num_layers, "num_neurons": num_neurons, "activation": activation, "optimizer": optimizer, "k_folds": k_folds})
                        
                        kf = StratifiedKFold(n_splits=k_folds, shuffle=True, random_state=42)
                        accuracies, losses = [], []

                        progress_bar = st.progress(0)# Kh·ªüi t·∫°o thanh tr·∫°ng th√°i ·ªü 0%
                        progress_text = st.empty()# T·∫°o m·ªôt v√πng tr·ªëng ƒë·ªÉ hi·ªÉn th·ªã % ti·∫øn tr√¨nh
                        
                        total_folds = k_folds
                        
                        for i, (train_idx, val_idx) in enumerate(kf.split(X_train, y_train)):
                            X_k_train, X_k_val = X_train[train_idx], X_train[val_idx]
                            y_k_train, y_k_val = y_train[train_idx], y_train[val_idx]
                            
                            cnn = keras.Sequential([layers.Input(shape=(X_k_train.shape[1],))] + [layers.Dense(num_neurons, activation=activation) for _ in range(num_layers)] + [layers.Dense(10, activation="softmax")])
                            cnn.compile(optimizer=optimizer, loss=loss_fn, metrics=["accuracy"])
                            progress_bar_epoch = st.progress(0)
                            
                            class EpochCallback(keras.callbacks.Callback):
                                def on_epoch_end(self, epoch, logs=None):
                                    progress_epoch = (epoch + 1) / epochs * 100
                                    progress_bar_epoch.progress(int(progress_epoch))
                                    st.write(f"Folds {i+1}/{k_folds}: Epoch {epoch+1}/{epochs}: ho√†n th√†nh :               Loss: {logs['loss']:.4f} , Accuracy: {logs['accuracy']:.4f}")

                            start_time = time.time()
                            history = cnn.fit(X_k_train, y_k_train, epochs=epochs, validation_data=(X_k_val, y_k_val), verbose=2, callbacks=[EpochCallback()])
                            # history = cnn.fit(X_k_train, y_k_train, epochs=epochs, validation_data=(X_k_val, y_k_val), verbose=2)
                            elapsed_time = time.time() - start_time
                            
                            accuracies.append(history.history["val_accuracy"][-1])
                            losses.append(history.history["val_loss"][-1])

                            # C·∫≠p nh·∫≠t thanh tr·∫°ng th√°i v√† hi·ªÉn th·ªã ph·∫ßn trƒÉm
                            progress = (i + 1) / total_folds  # T√≠nh ph·∫ßn trƒÉm ho√†n th√†nh
                            progress_bar.progress(progress)  # C·∫≠p nh·∫≠t thanh tr·∫°ng th√°i
                            progress_text.text(f"Ô∏èüéØTi·∫øn tr√¨nh hu·∫•n luy·ªán: {int(progress * 100)}%")  # Hi·ªÉn th·ªã % c·ª• th·ªÉ
                            
                        avg_val_accuracy = np.mean(accuracies)
                        avg_val_loss = np.mean(losses)
                        
                        mlflow.log_metrics({"avg_val_accuracy": avg_val_accuracy, "avg_val_loss": avg_val_loss, "elapsed_time": elapsed_time})
                        
                        test_loss, test_accuracy = cnn.evaluate(X_test, y_test, verbose=0)
                        mlflow.log_metrics({"test_accuracy": test_accuracy, "test_loss": test_loss})
                        mlflow.end_run()
                        st.session_state["trained_model"] = cnn
                        st.success(f"‚úÖ Hu·∫•n luy·ªán ho√†n t·∫•t!")
                        st.write(f"üìä **ƒê·ªô ch√≠nh x√°c trung b√¨nh tr√™n t·∫≠p validation:** {avg_val_accuracy:.4f}")
                        st.write(f"üìä **ƒê·ªô ch√≠nh x√°c tr√™n t·∫≠p test:** {test_accuracy:.4f}")


                        # Ghi log v·ªõi MLflow
                        mlflow.log_param("epochs", epochs)
                        mlflow.log_param("optimizer", optimizer)
                        mlflow.log_metric("train_accuracy", history.history['accuracy'][-1])
                        mlflow.log_metric("val_accuracy", history.history['val_accuracy'][-1])
                        mlflow.log_metric("final_train_loss", history.history['loss'][-1])
                        mlflow.log_metric("final_val_loss", history.history['val_loss'][-1])

                st.success("Hu·∫•n luy·ªán ho√†n t·∫•t!")
                st.write(f"Th·ªùi gian hu·∫•n luy·ªán: {elapsed_time:.2f} gi√¢y")
                st.write(f"ƒê·ªô ch√≠nh x√°c: {avg_val_accuracy:.4f}")

                # ƒê√°nh gi√° tr√™n t·∫≠p test
                test_loss, test_accuracy = cnn.evaluate(X_test, y_test, verbose=0)
                mlflow.log_metric("test_accuracy", test_accuracy)
                mlflow.log_metric("test_loss", test_loss)

                # L∆∞u model ƒë√£ hu·∫•n luy·ªán v√†o st.session_state
                st.session_state.selected_model_type = "Neural Network"
                st.session_state.trained_model = cnn
                st.session_state['history'] = history

                st.markdown("---")
                st.markdown("#### üìà**Bi·ªÉu ƒë·ªì Accuracy v√† Loss**")
                # V·∫Ω bi·ªÉu ƒë·ªì (x√≥a c√°c gi√° tr·ªã s·ªë)
                fig, (ax1, ax2) = plt.subplots(1, 2, figsize=(12, 4))
                
                # Bi·ªÉu ƒë·ªì Loss
                ax1.plot(history.history['loss'], label='Train Loss', color='blue')
                ax1.plot(history.history['val_loss'], label='Val Loss', color='orange')
                ax1.set_title('Loss')
                ax1.set_xlabel('Epoch')
                ax1.set_ylabel('Loss')
                ax1.legend()
                
                # Bi·ªÉu ƒë·ªì Accuracy
                ax2.plot(history.history['accuracy'], label='Train Accuracy', color='blue')
                ax2.plot(history.history['val_accuracy'], label='Val Accuracy', color='orange')
                ax2.set_title('Accuracy')
                ax2.set_xlabel('Epoch')
                ax2.set_ylabel('Accuracy')
                ax2.legend()

                st.pyplot(fig)

    with tab_demo:   
        with st.expander("**D·ª± ƒëo√°n k·∫øt qu·∫£**", expanded=True):
            st.write("**D·ª± ƒëo√°n tr√™n ·∫£nh do ng∆∞·ªùi d√πng t·∫£i l√™n**")

            # Ki·ªÉm tra xem m√¥ h√¨nh ƒë√£ ƒë∆∞·ª£c hu·∫•n luy·ªán v√† l∆∞u k·∫øt qu·∫£ ch∆∞a
            if "selected_model_type" not in st.session_state or "trained_model" not in st.session_state:
                st.warning("‚ö†Ô∏è Ch∆∞a c√≥ m√¥ h√¨nh n√†o ƒë∆∞·ª£c hu·∫•n luy·ªán. Vui l√≤ng hu·∫•n luy·ªán m√¥ h√¨nh tr∆∞·ªõc khi d·ª± ƒëo√°n.")
            else:
                best_model_name = st.session_state.selected_model_type
                best_model = st.session_state.trained_model

                st.write(f"M√¥ h√¨nh ƒëang s·ª≠ d·ª•ng: `{best_model_name}`")
                # st.write(f"‚úÖ ƒê·ªô ch√≠nh x√°c tr√™n t·∫≠p ki·ªÉm tra: `{st.session_state.get('test_accuracy', 'N/A'):.4f}`")

                # L·∫•y c√°c tham s·ªë t·ª´ session_state ƒë·ªÉ hi·ªÉn th·ªã

                # Cho ph√©p ng∆∞·ªùi d√πng t·∫£i l√™n ·∫£nh
                uploaded_file = st.file_uploader("üìÇ Ch·ªçn m·ªôt ·∫£nh ƒë·ªÉ d·ª± ƒëo√°n", type=["png", "jpg", "jpeg"])

                if uploaded_file is not None:
                    # ƒê·ªçc ·∫£nh t·ª´ t·ªáp t·∫£i l√™n
                    image = Image.open(uploaded_file).convert("L")  # Chuy·ªÉn sang ·∫£nh x√°m
                    image = np.array(image)

                    # Ki·ªÉm tra xem d·ªØ li·ªáu hu·∫•n luy·ªán ƒë√£ l∆∞u trong session_state hay ch∆∞a
                    if "X_train" in st.session_state:
                        X_train_shape = st.session_state["X_train"].shape[1]  # L·∫•y s·ªë ƒë·∫∑c tr∆∞ng t·ª´ t·∫≠p hu·∫•n luy·ªán

                        # Resize ·∫£nh v·ªÅ k√≠ch th∆∞·ªõc ph√π h·ª£p v·ªõi m√¥ h√¨nh ƒë√£ hu·∫•n luy·ªán
                        image = cv2.resize(image, (28, 28))  # C·∫≠p nh·∫≠t k√≠ch th∆∞·ªõc theo d·ªØ li·ªáu ban ƒë·∫ßu
                        image = image.reshape(1, -1)  # Chuy·ªÉn v·ªÅ vector 1 chi·ªÅu

                        # ƒê·∫£m b·∫£o s·ªë chi·ªÅu ƒë√∫ng v·ªõi d·ªØ li·ªáu hu·∫•n luy·ªán
                        if image.shape[1] == X_train_shape:
                            prediction = best_model.predict(image)[0]

                            # Hi·ªÉn th·ªã ·∫£nh v√† k·∫øt qu·∫£ d·ª± ƒëo√°n
                            st.image(uploaded_file, caption="üì∑ ·∫¢nh b·∫°n ƒë√£ t·∫£i l√™n", use_container_width=True)
                            
                            st.success(f"D·ª± ƒëo√°n: {np.argmax(prediction)} v·ªõi x√°c su·∫•t {np.max(prediction):.2f}")
                        else:
                            st.error(f"·∫¢nh kh√¥ng c√≥ s·ªë ƒë·∫∑c tr∆∞ng ƒë√∫ng ({image.shape[1]} thay v√¨ {X_train_shape}). H√£y ki·ªÉm tra l·∫°i d·ªØ li·ªáu ƒë·∫ßu v√†o!")
                    else:
                        st.error("D·ªØ li·ªáu hu·∫•n luy·ªán kh√¥ng t√¨m th·∫•y. H√£y hu·∫•n luy·ªán m√¥ h√¨nh tr∆∞·ªõc khi d·ª± ƒëo√°n.")

    with tab_demo_2:   
        with st.expander("**D·ª± ƒëo√°n k·∫øt qu·∫£**", expanded=True):
            st.write("**D·ª± ƒëo√°n tr√™n ·∫£nh do ng∆∞·ªùi d√πng t·∫£i l√™n**")

            # Ki·ªÉm tra xem m√¥ h√¨nh ƒë√£ ƒë∆∞·ª£c hu·∫•n luy·ªán v√† l∆∞u k·∫øt qu·∫£ ch∆∞a
            if "selected_model_type" not in st.session_state or "trained_model" not in st.session_state:
                st.warning("‚ö†Ô∏è Ch∆∞a c√≥ m√¥ h√¨nh n√†o ƒë∆∞·ª£c hu·∫•n luy·ªán. Vui l√≤ng hu·∫•n luy·ªán m√¥ h√¨nh tr∆∞·ªõc khi d·ª± ƒëo√°n.")
            else:
                best_model_name = st.session_state.selected_model_type
                best_model = st.session_state.trained_model

                st.write(f"M√¥ h√¨nh ƒëang s·ª≠ d·ª•ng: `{best_model_name}`")
                # st.write(f"‚úÖ ƒê·ªô ch√≠nh x√°c tr√™n t·∫≠p ki·ªÉm tra: `{st.session_state.get('test_accuracy', 'N/A'):.4f}`")

                # üÜï C·∫≠p nh·∫≠t key cho canvas khi nh·∫•n "T·∫£i l·∫°i"
                if "key_value" not in st.session_state:
                    st.session_state.key_value = str(random.randint(0, 1000000))

                if st.button("üîÑ T·∫£i l·∫°i"):
                    try:
                        st.session_state.key_value = str(random.randint(0, 1000000))
                    except Exception as e:
                        st.error(f"C·∫≠p nh·∫≠t key kh√¥ng th√†nh c√¥ng: {str(e)}")
                        st.stop()

                # ‚úçÔ∏è V·∫Ω d·ªØ li·ªáu
                canvas_result = st_canvas(
                    fill_color="black",
                    stroke_width=10,
                    stroke_color="white",
                    background_color="black",
                    height=150,
                    width=150,
                    drawing_mode="freedraw",
                    key=st.session_state.key_value,
                    update_streamlit=True
                ) 

                if st.button("D·ª± ƒëo√°n"):
                    img = preprocess_canvas_image(canvas_result)

                    if img is not None:
                        X_train = st.session_state["X_train"]
                        # Hi·ªÉn th·ªã ·∫£nh sau x·ª≠ l√Ω
                        st.image(Image.fromarray((img.reshape(28, 28) * 255).astype(np.uint8)), caption="·∫¢nh sau x·ª≠ l√Ω", width=100)

                        # D·ª± ƒëo√°n
                        prediction = best_model.predict(img)[0]

                        st.success(f"D·ª± ƒëo√°n: {np.argmax(prediction)} v·ªõi x√°c su·∫•t {np.max(prediction)*100:.2f}%")
                    else:
                        st.error("‚ö†Ô∏è H√£y v·∫Ω m·ªôt s·ªë tr∆∞·ªõc khi b·∫•m D·ª± ƒëo√°n!")

    with tab_mlflow:
        st.header("Th√¥ng tin Hu·∫•n luy·ªán & MLflow UI")
        try:
            client = MlflowClient()
            experiment_name = "Classification"
    
            # Ki·ªÉm tra n·∫øu experiment ƒë√£ t·ªìn t·∫°i
            experiment = client.get_experiment_by_name(experiment_name)
            if experiment is None:
                experiment_id = client.create_experiment(experiment_name)
                st.success(f"Experiment m·ªõi ƒë∆∞·ª£c t·∫°o v·ªõi ID: {experiment_id}")
            else:
                experiment_id = experiment.experiment_id
                st.info(f"ƒêang s·ª≠ d·ª•ng experiment ID: {experiment_id}")
    
            mlflow.set_experiment(experiment_name)
    
            # Truy v·∫•n c√°c run trong experiment
            runs = client.search_runs(experiment_ids=[experiment_id])
    
            # 1) Ch·ªçn v√† ƒë·ªïi t√™n Run Name
            st.subheader("ƒê·ªïi t√™n Run")
            if runs:
                run_options = {run.info.run_id: f"{run.data.tags.get('mlflow.runName', 'Unnamed')} - {run.info.run_id}"
                               for run in runs}
                selected_run_id_for_rename = st.selectbox("Ch·ªçn Run ƒë·ªÉ ƒë·ªïi t√™n:", 
                                                          options=list(run_options.keys()), 
                                                          format_func=lambda x: run_options[x])
                new_run_name = st.text_input("Nh·∫≠p t√™n m·ªõi cho Run:", 
                                             value=run_options[selected_run_id_for_rename].split(" - ")[0])
                if st.button("C·∫≠p nh·∫≠t t√™n Run"):
                    if new_run_name.strip():
                        client.set_tag(selected_run_id_for_rename, "mlflow.runName", new_run_name.strip())
                        st.success(f"ƒê√£ c·∫≠p nh·∫≠t t√™n Run th√†nh: {new_run_name.strip()}")
                    else:
                        st.warning("Vui l√≤ng nh·∫≠p t√™n m·ªõi cho Run.")
            else:
                st.info("Ch∆∞a c√≥ Run n√†o ƒë∆∞·ª£c log.")
    
            # 2) X√≥a Run
            st.subheader("Danh s√°ch Run")
            if runs:
                selected_run_id_to_delete = st.selectbox("", 
                                                         options=list(run_options.keys()), 
                                                         format_func=lambda x: run_options[x])
                if st.button("X√≥a Run", key="delete_run"):
                    client.delete_run(selected_run_id_to_delete)
                    st.success(f"ƒê√£ x√≥a Run {run_options[selected_run_id_to_delete]} th√†nh c√¥ng!")
                    st.experimental_rerun()  # T·ª± ƒë·ªông l√†m m·ªõi giao di·ªán
            else:
                st.info("Ch∆∞a c√≥ Run n√†o ƒë·ªÉ x√≥a.")
    
            # 3) Danh s√°ch c√°c th√≠ nghi·ªám
            st.subheader("Danh s√°ch c√°c Run ƒë√£ log")
            if runs:
                selected_run_id = st.selectbox("Ch·ªçn Run ƒë·ªÉ xem chi ti·∫øt:", 
                                               options=list(run_options.keys()), 
                                               format_func=lambda x: run_options[x])
    
                # 4) Hi·ªÉn th·ªã th√¥ng tin chi ti·∫øt c·ªßa Run ƒë∆∞·ª£c ch·ªçn
                selected_run = client.get_run(selected_run_id)
                st.write(f"**Run ID:** {selected_run_id}")
                st.write(f"**Run Name:** {selected_run.data.tags.get('mlflow.runName', 'Unnamed')}")
    
                st.markdown("### Tham s·ªë ƒë√£ log")
                st.json(selected_run.data.params)
    
                st.markdown("### Ch·ªâ s·ªë ƒë√£ log")
                metrics = {
                    "mean_cv_accuracy": selected_run.data.metrics.get("mean_cv_accuracy", "N/A"),
                    "std_cv_accuracy": selected_run.data.metrics.get("std_cv_accuracy", "N/A"),
                    "accuracy": selected_run.data.metrics.get("accuracy", "N/A"),
                    "model_type": selected_run.data.metrics.get("model_type", "N/A"),
                    "kernel": selected_run.data.metrics.get("kernel", "N/A"),
                    "C_value": selected_run.data.metrics.get("C_value", "N/A")
                

                }
                st.json(metrics)
    
                # 5) N√∫t b·∫•m m·ªü MLflow UI
                st.subheader("Truy c·∫≠p MLflow UI")
                mlflow_url = "https://dagshub.com/quangdinhhusc/HMVPYTHON.mlflow"
                if st.button("M·ªü MLflow UI"):
                    st.markdown(f'**[Click ƒë·ªÉ m·ªü MLflow UI]({mlflow_url})**')
            else:
                st.info("Ch∆∞a c√≥ Run n√†o ƒë∆∞·ª£c log. Vui l√≤ng hu·∫•n luy·ªán m√¥ h√¨nh tr∆∞·ªõc.")
    
        except Exception as e:
            st.error(f"Kh√¥ng th·ªÉ k·∫øt n·ªëi v·ªõi MLflow: {e}")

    


if __name__ == "__main__":
    run_NeuralNetwork_app()
    # st.write(f"MLflow Tracking URI: {mlflow.get_tracking_uri()}")
    # print("üéØ Ki·ªÉm tra tr√™n DagsHub: https://dagshub.com/Dung2204/MINST.mlflow/")
    # # # cd "C:\Users\Dell\OneDrive\Pictures\Documents\Code\python\OpenCV\HMVPYTHON\App"
    # ClassificationMinst.
    



    ## thay v√¨ decision tree l√† gini v√† entropy th√¨ -> ch·ªâ c√≤n entropy v·ªõi ch·ªçn ƒë·ªô s√¢u c·ªßa c√¢y
    ## b·ªï sung th√™m Ch·ªçn s·ªë folds (KFold Cross-Validation) ·ªü c·∫£ 2 ph·∫ßn decsion tree v√† svms
    ## c·∫≠p nh·∫≠t l·∫°i ph·∫ßn demo , v√¨ n√≥ ƒëang kh√¥ng s·ª≠ d·ª•ng d·ªØ li·ªáu ·ªü ph·∫ßn hu·∫•n luy·ªán
  
